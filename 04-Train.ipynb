{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corresponding-replication",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Finally we are going to train our atrium segmentation network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-bahamas",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "\n",
    "* pathlib for easy path handling\n",
    "* torch for tensor handling\n",
    "* pytorch lightning for efficient and easy training implementation\n",
    "* ModelCheckpoint and TensorboardLogger for checkpoint saving and logging\n",
    "* imgaug for Data Augmentation\n",
    "* numpy for file loading and array ops\n",
    "* matplotlib for visualizing some images\n",
    "* Our dataset and model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import CardiacDataset\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-implementation",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "Here we create the train and validation dataset. <br />\n",
    "Additionally, we define our data augmentation pipeline.\n",
    "Subsequently the two dataloaders are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broadband-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(scale=(0.85, 1.15),\n",
    "              rotate=(-45, 45)),\n",
    "    iaa.ElasticTransformation()\n",
    "])\n",
    "#aug_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laughing-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1932 train images and 339 val images\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset objects\n",
    "train_path = Path(\"Preprocessed/train/\")\n",
    "val_path = Path(\"Preprocessed/val\")\n",
    "\n",
    "train_dataset = CardiacDataset(train_path, seq)\n",
    "val_dataset = CardiacDataset(val_path, None)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "final-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-startup",
   "metadata": {},
   "source": [
    "## Custom Loss\n",
    "Often segmentation models perform better when using a Dice Loss instead of Cross-Entropy.<br />\n",
    "The Dice Loss is defined as:\n",
    "$$ L(\\hat{y}, y) = 1-\\frac{2 |\\hat{y} \\cap y|}{|\\hat{y}| + |y|}$$\n",
    "\n",
    "The intersection can be easily computed by $\\hat{y}$ * $y$ as both variables are binary masks.\n",
    "\n",
    "You can read more about the Dice Score here:\n",
    "https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorporate-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    class to compute the Dice Loss\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, mask): #ground mask \n",
    "                \n",
    "        # Flatten label and prediction tensors\n",
    "        pred = torch.flatten(pred) #eg: 8b*1C*256h*256w\n",
    "        mask = torch.flatten(mask)\n",
    "        counter = (pred * mask).sum()  # Numerator       \n",
    "        denum = pred.sum() + mask.sum() + 1e-8  # Denominator. Add a small number to prevent NANS\n",
    "        dice =  (2*counter)/denum\n",
    "        return 1 - dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-radio",
   "metadata": {},
   "source": [
    "## Full Segmentation Model\n",
    "\n",
    "We will now combine everything into the full pytorch lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "little-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtriumSegmentation(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = DiceLoss()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return torch.sigmoid(self.model(data))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mri, mask = batch\n",
    "        mask = mask.float()\n",
    "        pred = self(mri)\n",
    "        \n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        self.log(\"Train Dice\", loss)\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(mri.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mri, mask = batch\n",
    "        mask = mask.float()\n",
    "        pred = self(mri)\n",
    "        \n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        self.log(\"Val Dice\", loss)\n",
    "        \n",
    "        if batch_idx % 2 == 0:\n",
    "            self.log_images(mri.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def log_images(self, mri, pred, mask, name):\n",
    "        \n",
    "        pred = pred > 0.5\n",
    "        \n",
    "        fig, axis = plt.subplots(1, 2)\n",
    "        axis[0].imshow(mri[0][0], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(mask[0][0] == 0, mask[0][0])\n",
    "        axis[0].imshow(mask_, alpha=0.6)\n",
    "        \n",
    "        axis[1].imshow(mri[0][0], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(pred[0][0] == 0, pred[0][0])\n",
    "        axis[1].imshow(mask_, alpha=0.6)\n",
    "        \n",
    "        self.logger.experiment.add_figure(name, fig, self.global_step)#tensorboard\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer] #packed in list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "featured-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model and set the random seed\n",
    "torch.manual_seed(0)\n",
    "model = AtriumSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "diagnostic-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Dice',\n",
    "    save_top_k=10,\n",
    "    mode='min') #save 10 to that has min dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pacific-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "\n",
    "gpus = 1 #TODO\n",
    "trainer = pl.Trainer(logger=TensorBoardLogger(save_dir=\"./logs\"), log_every_n_steps=1,\n",
    "                     callbacks=checkpoint_callback,max_epochs=75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handy-testing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ./logs\\lightning_logs\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | model   | UNet     | 7.8 M \n",
      "1 | loss_fn | DiceLoss | 0     \n",
      "-------------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.127    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af676daefed543c788e56c11dbfcb551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-harvest",
   "metadata": {},
   "source": [
    "## Evaluation:\n",
    "Let's evaluate the model\n",
    "\n",
    "* nibabel to load a full scan\n",
    "* tqdm for progress bar when validating the model\n",
    "* celluloid for easy video generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affecting-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from tqdm.notebook import tqdm\n",
    "from celluloid import Camera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-tennis",
   "metadata": {},
   "source": [
    "First we load a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "unavailable-medline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.3.4 to v2.1.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\\Udemy-Medical PyTorch\\06-Atrium-Segmentation\\06-Atrium-Segmentation\\weights\\50.ckpt`\n"
     ]
    }
   ],
   "source": [
    "model = AtriumSegmentation.load_from_checkpoint(\"weights/50.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial-strain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval();\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-injection",
   "metadata": {},
   "source": [
    "Evalute the complete validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "imported-neighborhood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f33ab17b4db42329186dc277546097b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for slice, label in tqdm(val_dataset):\n",
    "    slice = torch.tensor(slice).to(device).unsqueeze(0)#channel dim added\n",
    "    with torch.no_grad(): #no grad as it is val\n",
    "        pred = model(slice)\n",
    "    preds.append(pred.cpu().numpy())\n",
    "    labels.append(label)\n",
    "    \n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-hometown",
   "metadata": {},
   "source": [
    "Compute overall Dice Score (1- DiceLoss):\n",
    "Wow! What a great score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sudden-password",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9510)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-model.loss_fn(torch.from_numpy(preds), torch.from_numpy(labels))  # two possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "weekly-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Val Dice Score is: 0.9509609341621399\n"
     ]
    }
   ],
   "source": [
    "dice_score = 1-DiceLoss()(torch.from_numpy(preds), torch.from_numpy(labels).unsqueeze(0).float())\n",
    "print(f\"The Val Dice Score is: {dice_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-extent",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-batman",
   "metadata": {},
   "source": [
    "We can now load a test subject from the dataset and estimate the position of the left atrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "prime-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = Path(\"Task02_Heart/imagesTs/la_001.nii.gz\")\n",
    "subject_mri = nib.load(subject).get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-symphony",
   "metadata": {},
   "source": [
    "As this scan is neither normalized nor standardized we need to perform those tasks!<br />\n",
    "Let us copy the normalization and standardization functions from our preprocessing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "frequent-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for normalization and standardization\n",
    "def normalize(full_volume):\n",
    "    \"\"\"\n",
    "    Z-Normalization of the whole subject\n",
    "    \"\"\"\n",
    "    mu = full_volume.mean()\n",
    "    std = np.std(full_volume)\n",
    "    normalized = (full_volume - mu) / std\n",
    "    return normalized\n",
    "\n",
    "def standardize(normalized_data):\n",
    "    \"\"\"\n",
    "    Standardize the normalized data into the 0-1 range\n",
    "    \"\"\"\n",
    "    standardized_data = (normalized_data - normalized_data.min()) / (normalized_data.max() - normalized_data.min())\n",
    "    return standardized_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-medicine",
   "metadata": {},
   "source": [
    "We preprocess the scan and crop 32 px from top, bottom, back and front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "defensive-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_mri = subject_mri[32:-32, 32:-32]\n",
    "standardized_scan = standardize(normalize(subject_mri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "enormous-clearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 336, 180)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "civilian-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(standardized_scan.shape[-1]): #saggital\n",
    "    slice = standardized_scan[:,:,i] #current slice\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(slice).unsqueeze(0).unsqueeze(0).float().to(device))[0][0] \n",
    "        #channel dim and batch dim aftr copying again remove batch and channel dim\n",
    "        pred = pred > 0.5\n",
    "    preds.append(pred.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "minute-student",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # create the camera object from celluloid\n",
    "\n",
    "for i in range(standardized_scan.shape[-1]):#saggital\n",
    "    plt.imshow(standardized_scan[:,:,i], cmap=\"bone\")\n",
    "    mask = np.ma.masked_where(preds[i]==0, preds[i])\n",
    "    plt.imshow(mask, alpha=0.5, cmap=\"autumn\")\n",
    "    \n",
    "    camera.snap()  # Store the current slice\n",
    "animation = camera.animate()  # create the animation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "falling-legislature",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.animation' has no attribute 'to_jshtml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[1;32m----> 2\u001b[0m HTML(\u001b[43manimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_jshtml\u001b[49m())  \u001b[38;5;66;03m# convert the animation to a video\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.animation' has no attribute 'to_jshtml'"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(animation.to_jshtml())  # convert the animation to a video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0f2a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!br install ffmpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-function",
   "metadata": {},
   "source": [
    "Congratulations! You made it to the end of the segmentation lecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
